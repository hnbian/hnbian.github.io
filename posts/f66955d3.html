<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Spark Streaming 2. DStream 介绍, hnbian">
    <meta name="description" content="1. DStreams(离散数据流)离散数据流(DStream) 是spark Streaming最基本的抽象，它代表了一种连续的数据流，要么从某种数据源提取数据，要么从其他数据流映射转换而来。DStream内部是由一系列连续的RDD 组成">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-155985521-1"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());
    gtag('config', 'UA-155985521-1');
</script>


    <title>Spark Streaming 2. DStream 介绍 | hnbian</title>
    <link rel="icon" type="image/png" href="https://images.hnbian.cn/FvNSAGx7xljxALFh8Fa2DPNFP-JB">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="hnbian" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="stylesheet" href="/css/prism-okaidia.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="https://images.hnbian.cn/FvNSAGx7xljxALFh8Fa2DPNFP-JB" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">hnbian</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="https://images.hnbian.cn/FvNSAGx7xljxALFh8Fa2DPNFP-JB" class="logo-img circle responsive-img">
        
        <div class="logo-name">hnbian</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/hnbian" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/hnbian" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://images.hnbian.cn/Fm-RctU4WjbVW6A1YsXiG73hE54H')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Spark Streaming 2. DStream 介绍</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/spark/">
                                <span class="chip bg-color">spark</span>
                            </a>
                        
                            <a href="/tags/spark-streaming/">
                                <span class="chip bg-color">spark streaming</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/spark/" class="post-category">
                                spark
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2018-02-26
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="1-DStreams-离散数据流"><a href="#1-DStreams-离散数据流" class="headerlink" title="1. DStreams(离散数据流)"></a>1. DStreams(离散数据流)</h2><p>离散数据流(DStream) 是spark Streaming最基本的抽象，它代表了一种连续的数据流，要么从某种数据源提取数据，要么从其他数据流映射转换而来。DStream内部是由一系列连续的RDD 组成的，每个RDD都是不可变、分布式的数据集,(详见spark编程指南 <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds">Spark Programming Guide</a> )。每个RDD都包含了特定的时间间隔内的一批数据，如下如所示:</p>
<img src="https://images.hnbian.cn/Fg4wP8qlsYSrGnqy3MzzdPQtSvsw?imageView2/0/interlace/1/q/70%7Cwatermark/2/text/d3d3LmhuYmlhbi5jbg==/font/5b6u6L2v6ZuF6buR/fontsize/600/fill/IzBDMjU2NA==/dissolve/100/gravity/SouthEast/dx/10/dy/10%7Cimageslim" style="zoom:50%;">

<p>任何作用于DStream的算子,其内部都会被转化为对内部RDD 的操作。例如，在前面的例子中，我们将lines 这个DStream转化成words DSsream对象，其实作用与lines 上的flatMap 算子，会施加于lines 中的每个RDD 上面，并生成新的对应的RDD，而这些新城省的RDD 对象就组成了words 这个DStream对象，底层的 RDD 转换仍是由spark引擎计算的， DStream的算子将这些细节隐藏了起来，并未开发者提供了更方便的高级API。过程如下图：</p>
<img src="https://images.hnbian.cn/Fq4U9OMjljMp06FIGjlN9fQQZEMw?imageView2/0/interlace/1/q/70%7Cwatermark/2/text/d3d3LmhuYmlhbi5jbg==/font/5b6u6L2v6ZuF6buR/fontsize/600/fill/IzBDMjU2NA==/dissolve/100/gravity/SouthEast/dx/10/dy/10%7Cimageslim" style="zoom:50%;">



<h2 id="2-DStream-Receivers"><a href="#2-DStream-Receivers" class="headerlink" title="2. DStream Receivers"></a>2. DStream Receivers</h2><p>Spark Streaming主要提供了两种内建流式数据源</p>
<ul>
<li>基础数据源(basic sources): 在StreamingContext API中可以直接使用，如: file system、 socket 或者 AKKA actor。</li>
<li>高级数据源(Advanced sources): 需要额外工具类的源，如:kafka、flume、kinesis、Twitter等，这些数据源都需要增加额外的依赖</li>
</ul>
<p>如果你需要同时从过个数据源拉取数据，那么你就需要创建多个DStream对象。 多个DStream对象其实也就同时创建了多个数据流接收器，但是需要注意的是Spark的worker/executor都是长期运行的，因此他们都会各自占用一个分配给SparkStreaming应用的CPU，所以在运行SparkStreaming应用的时候,需要注意分片足够的CPU core(本地运行时,需要足够的线程)来处理接收到的数据，同时还要足够的CPUcore来运行这些Receivers。</p>
<p>如果本地运行spark Streaming应用，记得不能将master设置为”local”     或者 “local[1]” ，这两个值都只会在本地启动一个线程。而如果此时你使用一个包含接收器(如：socket,kafka,flume等) 的输入 Dstream ，那么这个线程只能用于运行这个接收器，而处理数据的逻辑就没有线程来执行了，因此，本地运行时，一定要将master设置为”local[n]”，    其中 n &gt; 接收器的个数(有关master的详情请参考 <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/configuration.html#spark-properties">Spark Properties</a>)</p>
<p>将Spark Streaming 应用置于集群中运行时，同样分配给应用的CPU core数必须大于接收器的总数，否则该应用就只会接收数据，而不会处理数据</p>
<h3 id="2-1-基础数据源"><a href="#2-1-基础数据源" class="headerlink" title="2.1 基础数据源"></a>2.1 基础数据源</h3><p>在前面的例子中我们已经看到，使用ssc.socketTextStream(…) 可以从TCP连接重接收文本数据，而除了TCP socket外，StreamingContext API 还支持从文件或者AKKA actor中拉去数据</p>
<ul>
<li>文件数据流(File Streams)</li>
</ul>
<p>可以从任何可兼容的HDFS API (hdfs，s3，nfs …) 的文件系统，创建方式如下</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">streamingContext.fileStream[<span class="type">KeyClass</span>, <span class="type">ValueClass</span>, <span class="type">InputFormatClass</span>](dataDirectory)</span><br></pre></td></tr></tbody></table></figure>



<p>SparkStreaming 将监视该dataDirectory目录，并处理该目录下任何新建的文件(目前还不支持嵌套目录),注意:</p>
<ul>
<li>各个文件数据格式必须一致</li>
<li>dataDirectory 中的文件必须通过moving 或者renaming 来创建</li>
<li>一旦文件move 进dataDirectory 之后，就不能再改动，所以如果这个文件后续还有写入，这些新写入的数据不会被读取.</li>
</ul>
<p>对于简单的文本文件，更简单的方式是调用 StreamingContext.textFileStream(dataDirectory)</p>
<p>另外,文件数据流不是基于接收器的，所以不需要为其单独分片一个CPU core</p>
<p>Python API fileStream 目前暂时不可用，Python目前只支持testFileStream</p>
<ul>
<li>RDD队列数据流(Queue of RDDs as a Stream)</li>
</ul>
<p>如果需要测试SparkStreaming应用，你可以创建一个基于一批RDD的DStream对象,只需要调用streamingContext.queueStream(queueOfRdds)，RDD会被一个个依次推入队列，而DStream则会依次以数据流形式处理这些RDD的数据</p>
<p>关于套接字(socket)，文件 以及akka actor 数据流更详细信息,请参考文档:<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.StreamingContext">StreamingContext</a> for Scala,<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html">JavaStreamingContext</a> for Java，and <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext">StreamingContext</a> for Python。</p>
<h3 id="2-2-高级数据源"><a href="#2-2-高级数据源" class="headerlink" title="2.2 高级数据源"></a>2.2 高级数据源</h3><p>自 Spark1.6.1 起 Python API 将支持 kafka,kinesis，flume,mqtt 这些数据源。</p>
<p>使用这类数据源需要以来一些额外的代码库,有些依赖还挺复杂(如:kafka,flume)，因此为了减少依赖项版本冲突,各个数据源DStream的相关功能被分割到不同的代码包中,只有用到的时候才需要链接打包进来,例如你需要使用Twitter的tweets 作为数据源,可以参考一下步骤：</p>
<ol>
<li>linking : 将spark-streaming-twitter_2.10 依赖加入到项目依赖中</li>
<li>Programming: 导入TwitterUtils 类，然后调用TwitterUtils.createStream     创建一个Dstream</li>
</ol>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.twitter._</span><br><span class="line"></span><br><span class="line"><span class="type">TwitterUtils</span>.createStream(ssc, <span class="type">None</span>)</span><br></pre></td></tr></tbody></table></figure>



<ol start="3">
<li>deploying: 生成一个uber jar包,并包含其所有依赖项(包括spark-streaming-twitter_2.10及其自身依赖树)，再部署这个jar包,部署详见(<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#deploying-applications">Deploying section</a>)</li>
</ol>
<p>高级数据源在spark-shell 中不可用,因此不能用spark-shell来测试基于高级数据源的应用,如果真的需要的话,你需要自行下载对应的数据源的maven工具及其依赖,并将这个jar部署到spark-shell 的classpath中</p>
<p>下面列举了一些高级数据源</p>
<ul>
<li>Kafka: Spark Streaming 1.6.1 可兼容     Kafka 0.8.2.1。详见<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/streaming-kafka-integration.html">Kafka Integration Guide</a>。</li>
<li>Flume: Spark Streaming 1.6.1 可兼容     Flume 1.6.0 。详见<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/streaming-flume-integration.html">Flume Integration Guide</a>。</li>
<li>Kinesis: Spark Streaming 1.6.1 可兼容     Kinesis Client Library 1.2.1。详见<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/streaming-kinesis-integration.html">Kinesis Integration Guide</a>。</li>
<li>Twitter: Spark Streaming     TwitterUtils 使用Twitter4j 通过 <a target="_blank" rel="noopener" href="https://dev.twitter.com/docs/streaming-apis">Twitter’s Streaming API</a>     拉取公开tweets数据流。认证信息可以用任何Twitter4j所支持的方法（<a target="_blank" rel="noopener" href="http://twitter4j.org/en/configuration.html">methods</a>）。你可以获取所有的公开数据流，当然也可以基于某些关键词进行过滤。示例可以参考<a target="_blank" rel="noopener" href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/TwitterPopularTags.scala">TwitterPopularTags</a> 和 <a target="_blank" rel="noopener" href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/TwitterAlgebirdCMS.scala">TwitterAlgebirdCMS</a>。</li>
</ul>
<h4 id="2-2-1-Spark-streaming-kafka-代码示例"><a href="#2-2-1-Spark-streaming-kafka-代码示例" class="headerlink" title="2.2.1 Spark-streaming kafka 代码示例"></a>2.2.1 Spark-streaming kafka 代码示例</h4><ul>
<li>添加依赖</li>
</ul>
<figure class="highlight xml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>



<ul>
<li>代码</li>
</ul>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.{<span class="type">Seconds</span>, <span class="type">StreamingContext</span>}</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by hnbia on 2017/4/4.</span></span><br><span class="line"><span class="comment">  * spark streaming -kafka</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming_Kafka</span> </span>{</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = {</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"SparkStreaming_Kafka"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="comment">//制定kafka的broker 信息以及消费者信息</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="string">"bootstrap.servers"</span> -&gt; <span class="string">"master1:9092,master2:9092,slave1:9092"</span>,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="string">"group.id"</span> -&gt; <span class="string">"mygroupkafka"</span>,</span><br><span class="line">      <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"latest"</span>,</span><br><span class="line">      <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//topic 集合 可以指定多个topic</span></span><br><span class="line">    <span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">"test"</span>)</span><br><span class="line">    <span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    stream.map(record =&gt; (record.key, record.value)).print()</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>



<h4 id="2-2-2-spark-streaming-flume-代码示例"><a href="#2-2-2-spark-streaming-flume-代码示例" class="headerlink" title="2.2.2 spark-streaming flume 代码示例"></a>2.2.2 spark-streaming flume 代码示例</h4><ul>
<li>Maven 依赖</li>
</ul>
<figure class="highlight xml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-flume_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure>



<ul>
<li>配置flume 使用avro sink</li>
</ul>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">定义<span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 8888</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">定义channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">定义sink</span></span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = localhost</span><br><span class="line">a1.sinks.k1.port = 9999</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">绑定</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.channel = c1</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>代码</li>
</ul>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.{<span class="type">Seconds</span>, <span class="type">StreamingContext</span>}</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.flume._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by hnbia on 2017/4/4.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming_Flume</span> </span>{</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = {</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"SparkStreaming_Flume"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="keyword">val</span> flumeStream = <span class="type">FlumeUtils</span>.createStream(ssc, <span class="string">"master1"</span>, <span class="number">9999</span>)</span><br><span class="line">    flumeStream.map(e =&gt; e.event.getBody.toString)</span><br><span class="line">    println(flumeStream)</span><br><span class="line">  }</span><br><span class="line">}</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>



<h4 id="2-2-3-Spark-streaming-kafka-flume-整合"><a href="#2-2-3-Spark-streaming-kafka-flume-整合" class="headerlink" title="2.2.3 Spark-streaming kafka flume 整合"></a>2.2.3 Spark-streaming kafka flume 整合</h4><ul>
<li>配置 flume</li>
</ul>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">定义<span class="built_in">source</span></span></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 8888</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">定义channel</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">定义sink</span></span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k1.kafka.topic = test</span><br><span class="line">a1.sinks.k1.kafka.bootstrap.servers = slave1:9092</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">绑定</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.channel = c</span><br></pre></td></tr></tbody></table></figure>



<ul>
<li>Spark-Streaming 代码示例</li>
</ul>
<p>Window length：跨多少Dstream (必须是batch interval倍数)</p>
<p>Window slide：滑动长度（必须是batch interval倍数，多久进行窗口操作）</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.{<span class="type">Seconds</span>, <span class="type">StreamingContext</span>}</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by hnbia on 2017/4/4.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming_Window</span> </span>{</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = {</span><br><span class="line">      println(<span class="string">"开始 流处理 。 。 。"</span>)</span><br><span class="line">      <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"SparkStreamingDemo1"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">      <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">      <span class="comment">//设置保存点 用于故障恢复</span></span><br><span class="line">      <span class="comment">//ssc.checkpoint("/tmp/spark/checkpoint")</span></span><br><span class="line">      ssc.checkpoint(<span class="string">"D:\\spark"</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">//创建离散流，</span></span><br><span class="line">      <span class="keyword">val</span> streamLines = ssc.socketTextStream(<span class="string">"master1"</span>, <span class="number">9999</span>,<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">      <span class="comment">// 按照窗口进行消息计数 以及打印</span></span><br><span class="line">      streamLines</span><br><span class="line">        .flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        .map((_,<span class="number">1</span>))</span><br><span class="line">        .reduceByKeyAndWindow((a:<span class="type">Int</span>,b:<span class="type">Int</span>)=&gt;{a+b},<span class="type">Seconds</span>(<span class="number">15</span>),<span class="type">Seconds</span>(<span class="number">10</span>))</span><br><span class="line">        .print()</span><br><span class="line">      <span class="comment">//streamLines.reduceByWindow((_+_),Seconds(15),Seconds(10))</span></span><br><span class="line">      <span class="comment">// 启动流计算</span></span><br><span class="line">      ssc.start()</span><br><span class="line">      <span class="comment">// 等待结束</span></span><br><span class="line">      ssc.awaitTermination()</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>



<h3 id="2-3-接收器的可靠性"><a href="#2-3-接收器的可靠性" class="headerlink" title="2.3 接收器的可靠性"></a>2.3 接收器的可靠性</h3><p>从可靠性角度划分，大致有两种数据源,其中kafka,flume这样的数据源，他们支持对所传输的数据进行确认，系统受到这类可靠数据源过来的数据，然后发出确认信息,这样就能够保证任何失败的情况下都不会丢失数据,因此我们可以将接收器相应地分为两类:</p>
<p>1.可靠接收器(Reliable Receiver) - 可靠接收器会在成功接收并保存好Spark数副本后,向可靠数据源发送确认信息</p>
<p>2.不可靠接收器(Unreliable Receiver) - 不可靠接收器不会发送任何确认信息,不过这种接收器常用于不支持确认的数据源，或者不想引入确认数据的复杂性的数据源</p>
<h2 id="3-DStream-支持的算子"><a href="#3-DStream-支持的算子" class="headerlink" title="3. DStream 支持的算子"></a>3. DStream 支持的算子</h2><h3 id="3-1-转换算子"><a href="#3-1-转换算子" class="headerlink" title="3.1 转换算子"></a>3.1 转换算子</h3><p>和RDD类似，DStream也支持从输入DStream经过各种transformation 算子映射成新的Dstream，DStream支持很多RDD 常见的transformation算子,常用的见下表</p>
<table>
<thead>
<tr>
<th>Transformation算子</th>
<th>用途</th>
</tr>
</thead>
<tbody><tr>
<td>map(func)</td>
<td>将源DStream中每个元素通过func映射为新的元素  ，返回会一个新的DStream</td>
</tr>
<tr>
<td>flatMap(func)</td>
<td>和map类似，不过每个输入元素不再是映射为一个输出，而是映射为0到多个输出</td>
</tr>
<tr>
<td>filter(func)</td>
<td>过滤经过 func 函数计算之后返回 true 的元素，返回一个新的DStream</td>
</tr>
<tr>
<td>repartition(numPartitions)</td>
<td>更改DStream的并行度，增加或减少分区数</td>
</tr>
<tr>
<td>union(otherStream)</td>
<td>返回新的DStream，包含源DStream和otherDStream元素的并集</td>
</tr>
<tr>
<td>count()</td>
<td>返回一个包含单元素RDDs的DStream，其中每个元素是源DStream中各个RDD中的元素个数</td>
</tr>
<tr>
<td>reduce(func)</td>
<td>返回一个包含单元素RDDs的DStream，其中每个元素是通过源RDD中各个RDD的元素经func（func输入两个参数并返回一个同类型结果数据）聚合得到的结果。func必须满足结合律，以便支持并行计算。</td>
</tr>
<tr>
<td>countByValue()</td>
<td>如果源DStream包含的元素类型为K，那么该算子返回新的DStream包含元素为(K,  Long)键值对，其中K为源DStream各个元素，而Long为该元素出现的次数。</td>
</tr>
<tr>
<td>reduceByKey(func,  [numTasks])</td>
<td>如果源DStream 包含的元素为 (K,  V) 键值对，则该算子返回一个新的也包含(K,  V)键值对的DStream，其中V是由func聚合得到的。注意：默认情况下，该算子使用Spark的默认并发任务数（本地模式为2，集群模式下由spark.default.parallelism  决定）。你可以通过可选参数numTasks来指定并发任务个数。</td>
</tr>
<tr>
<td>join(otherStream,  [numTasks])</td>
<td>如果源DStream包含元素为(K,  V)，同时otherDStream包含元素为(K,  W)键值对，则该算子返回一个新的DStream，其中源DStream和otherDStream中每个K都对应一个 (K, (V, W))键值对元素。</td>
</tr>
<tr>
<td>cogroup(otherStream,  [numTasks])</td>
<td>如果源DStream包含元素为(K,  V)，同时otherDStream包含元素为(K, W)键值对，则该算子返回一个新的DStream，其中每个元素类型为包含(K, Seq[V]， Seq[W])的tuple。</td>
</tr>
<tr>
<td>transform(func)</td>
<td>返回一个新的DStream，其包含的RDD为源RDD经过func操作后得到的结果。利用该算子可以对DStream施加任意的操作。</td>
</tr>
<tr>
<td>updateStateByKey(func)</td>
<td>返回一个包含新”状态”的DStream。源DStream中每个key及其对应的values会作为func的输入，而func可以用于对每个key的“状态”数据作任意的更新操作。</td>
</tr>
</tbody></table>
<ul>
<li>UpdateStateByKey 算子</li>
</ul>
<p>updateStateByKey算子支持维护一个任意的状态，要实现这一点,只需要两步:</p>
<p>1.定义状态 - 状态数据可以是任意类型</p>
<p>2.定义状态更新函数 - 定义好一个函数，其输入为数据流之前的状态和新的数据流数据，且可更新步骤1 中定义的输入数据流的状态.</p>
<p>在每一批次数据到达后,spark都会调用状态更新函数来更新所有已有key 的状态.(不管key 是否存在本批次中) 如果状态更新函数返回None，则对应的键值对会被删除</p>
<p>举例如下，假设你需要维护一个流式应用，统计数据流中每个单词出现的次数，这里将各个单词出现的次数这个整数定义为状态，我们接下来定义状态更新函数</p>
<p>该状态更新函数会为每个单词调用一次,且响应的newValues 是一个包含很多个 “1” 的数组(这些1 来自于(word,1) 键值对)，而runningCount包含之前该单词的计数，</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StatefulNetworkWordCount</span> </span>{</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) {</span><br><span class="line">    <span class="keyword">if</span> (args.length &lt; <span class="number">2</span>) {</span><br><span class="line">      <span class="type">System</span>.err.println(<span class="string">"Usage: StatefulNetworkWordCount &lt;hostname&gt; &lt;port&gt;"</span>)</span><br><span class="line">      <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">//StreamingExamples.setStreamingLogLevels()</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"StatefulNetworkWordCount"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="comment">// 创建streamingContext 每5秒一个批次</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    ssc.checkpoint(<span class="string">"d:/data"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initial state RDD for mapWithState operation</span></span><br><span class="line">    <span class="keyword">val</span> initialRDD = ssc.sparkContext.parallelize(<span class="type">List</span>((<span class="string">"hello"</span>, <span class="number">1</span>), (<span class="string">"world"</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create a ReceiverInputDStream on target ip:port and count the</span></span><br><span class="line">    <span class="comment">// words in input stream of \n delimited test (eg. generated by 'nc')</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(args(<span class="number">0</span>), args(<span class="number">1</span>).toInt)</span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordDstream = words.map(x =&gt; (x, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Update the cumulative count using mapWithState</span></span><br><span class="line">    <span class="comment">// This will give a DStream made of state (which is the cumulative count of the words)</span></span><br><span class="line">    <span class="keyword">val</span> mappingFunc = (word: <span class="type">String</span>, one: <span class="type">Option</span>[<span class="type">Int</span>], state: <span class="type">State</span>[<span class="type">Int</span>]) =&gt; {</span><br><span class="line">      <span class="keyword">val</span> sum = one.getOrElse(<span class="number">0</span>) + state.getOption.getOrElse(<span class="number">0</span>)</span><br><span class="line">      <span class="keyword">val</span> output = (word, sum)</span><br><span class="line">      state.update(sum)</span><br><span class="line">      output</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stateDstream = wordDstream.mapWithState(<span class="type">StateSpec</span>.function(mappingFunc).initialState(initialRDD))</span><br><span class="line">    stateDstream.print()</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>transform 算子(及其变体transformWith) 可以支持任意RDF到RDD的映射操作，也就是说，你可以用transform算子来包装任何Dstream 不支持的RDD算子，例如，将DStream每个批次中的RDD和另一个DataSet 进行关联(join) 操作，这个功能Dstream API并没有直接支持，不过你可以用transform来实现这个功能，可见 transform其实为Dstream 提供了非常强大的功能支持，比如说你可以用事先算好的信息，对DStream进行实时过滤</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">val</span> spamInfoRDD = ssc.sparkContext.newAPIHadoopRDD(...) <span class="comment">// 包含垃圾信息的RDD</span></span><br><span class="line"><span class="keyword">val</span> cleanedDStream = wordCounts.transform(rdd =&gt; {</span><br><span class="line">  rdd.join(spamInfoRDD).filter(...) <span class="comment">// 将DStream中的RDD和spamInfoRDD关联，并实时过滤垃圾数据</span></span><br><span class="line">  ...</span><br><span class="line">})</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<p>注意，这里transform包含的算子，其调用的时间间隔和批次间隔是相同的，所以你可以基于时间改变对RDD的操作，如: 在不同批次，调用不同的RDD算子，设置不同的RDD分区或者广播变量等</p>
<h3 id="3-2-窗口算子"><a href="#3-2-窗口算子" class="headerlink" title="3.2 窗口算子"></a>3.2 窗口算子</h3><p>spark streaming 同样也提供基于时间窗口的计算，也就是说，你可以对某一个滑动时间窗口内的数据施加特定的transformation算子。</p>
<p><img src="https://images.hnbian.cn/FhxkNArSIsXJus6MLq1COak-Qjr5?imageView2/0/interlace/1/q/70%7Cwatermark/2/text/d3d3LmhuYmlhbi5jbg==/font/5b6u6L2v6ZuF6buR/fontsize/600/fill/IzBDMjU2NA==/dissolve/100/gravity/SouthEast/dx/10/dy/10%7Cimageslim"></p>
<p>每次窗口滑动时，源DStream中落入窗口的RDDs就会被合并成新的windowed Dstream ，在上图的例子中，这个操作会施加于三个RDD单元，而滑动距离是两个RDD 单元，由此可以得出任何窗口的相关操作都需要执行以下两个参数:</p>
<ol>
<li><p>(窗口长度) window length - 窗口覆盖的时间长度，多长时间进行一次计算(上图中为3)</p>
</li>
<li><p>(滑动距离) sliding interval - 窗口启动的时间间隔(上图中为2)</p>
</li>
</ol>
<p>注意这两个参数都必须是DStream批次间隔的整数倍数</p>
<p>举例，加入，你需要扩展前面的代码，需要每个十秒同意下前30秒内的单词计数，为此我们需要在包含(word,1) 键值对的DStream上，对最近30秒的数据调用reduceByKey算子，不过这些都是可以简单的用一个reduceBykeyAndWindow 来搞定</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> windowedWordCounts = pairs.reduceByKeyAndWindow((a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; (a + b), <span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>))</span><br></pre></td></tr></tbody></table></figure>



<ul>
<li>以下列出了常用的窗口算子，这些算子都有前面提到的两个参数，- 窗口长度和滑动距离.</li>
</ul>
<table>
<thead>
<tr>
<th>Transformation窗口算子</th>
<th>用途</th>
</tr>
</thead>
<tbody><tr>
<td>window(windowLength, slideInterval)</td>
<td>将源DStream窗口化，并返回转化后的DStream</td>
</tr>
<tr>
<td>countByWindow(windowLength,slideInterval)</td>
<td>返回数据流在一个滑动窗口内的元素个数</td>
</tr>
<tr>
<td>reduceByWindow(func, windowLength,slideInterval)</td>
<td>基于数据流在一个滑动窗口内的元素，用func做聚合，返回一个单元素数据流。func必须满足结合律，以便支持并行计算。</td>
</tr>
<tr>
<td>reduceByKeyAndWindow(func,windowLength, slideInterval,  [numTasks])</td>
<td>基于(K,  V)键值对DStream，将一个滑动窗口内的数据进行聚合，返回一个新的包含(K,V)键值对的DStream，其中每个value都是各个key经过func聚合后的结果。  注意：如果不指定numTasks，其值将使用Spark的默认并行任务数（本地模式下为2，集群模式下由  spark.default.parallelism决定）。当然，你也可以通过numTasks来指定任务个数。</td>
</tr>
<tr>
<td>reduceByKeyAndWindow(func, invFunc,windowLength,slideInterval,  [numTasks])</td>
<td>和前面的reduceByKeyAndWindow()  类似，只是这个版本会用之前滑动窗口计算结果，递增地计算每个窗口的归约结果。当新的数据进入窗口时，这些values会被输入func做归约计算，而这些数据离开窗口时，对应的这些values又会被输入  invFunc  做”反归约”计算。举个简单的例子，就是把新进入窗口数据中各个单词个数“增加”到各个单词统计结果上，同时把离开窗口数据中各个单词的统计个数从相应的统计结果中“减掉”。不过，你的自己定义好”反归约”函数，即：该算子不仅有归约函数（见参数func），还得有一个对应的”反归约”函数（见参数中的  invFunc）。和前面的reduceByKeyAndWindow()  类似，该算子也有一个可选参数numTasks来指定并行任务数。注意，这个算子需要配置好检查点（<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing">checkpointing</a>）才能用。</td>
</tr>
<tr>
<td>countByValueAndWindow(windowLength,slideInterval,  [numTasks])</td>
<td>基于包含(K,  V)键值对的DStream，返回新的包含(K, Long)键值对的DStream。其中的Long value都是滑动窗口内key出现次数的计数。  和前面的reduceByKeyAndWindow()  类似，该算子也有一个可选参数numTasks来指定并行任务数。</td>
</tr>
</tbody></table>
<ul>
<li>代码示例</li>
</ul>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.{<span class="type">Seconds</span>, <span class="type">StreamingContext</span>}</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by hnbia on 2017/4/4.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming_Window</span> </span>{</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = {</span><br><span class="line">      println(<span class="string">"开始 流处理 。 。 。"</span>)</span><br><span class="line">      <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"SparkStreamingDemo1"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">      <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">      <span class="comment">//设置保存点 用于故障恢复</span></span><br><span class="line">      <span class="comment">//ssc.checkpoint("/tmp/spark/checkpoint")</span></span><br><span class="line">      ssc.checkpoint(<span class="string">"D:\\spark"</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">//创建离散流，</span></span><br><span class="line">      <span class="keyword">val</span> streamLines = ssc.socketTextStream(<span class="string">"master1"</span>, <span class="number">9999</span>,<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">      <span class="comment">// 按照窗口进行消息计数 以及打印</span></span><br><span class="line">      streamLines</span><br><span class="line">        .flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        .map((_,<span class="number">1</span>))</span><br><span class="line">        .reduceByKeyAndWindow((a:<span class="type">Int</span>,b:<span class="type">Int</span>)=&gt;{a+b},<span class="type">Seconds</span>(<span class="number">15</span>),<span class="type">Seconds</span>(<span class="number">10</span>))</span><br><span class="line">        .print()</span><br><span class="line">      <span class="comment">//streamLines.reduceByWindow((_+_),Seconds(15),Seconds(10))</span></span><br><span class="line">      <span class="comment">// 启动流计算</span></span><br><span class="line">      ssc.start()</span><br><span class="line">      <span class="comment">// 等待结束</span></span><br><span class="line">      ssc.awaitTermination()</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>



<h3 id="3-3-Join-算子"><a href="#3-3-Join-算子" class="headerlink" title="3.3 Join 算子"></a>3.3 Join 算子</h3><h4 id="3-3-1-数据流与数据流做关联"><a href="#3-3-1-数据流与数据流做关联" class="headerlink" title="3.3.1 数据流与数据流做关联"></a>3.3.1 数据流与数据流做关联</h4><p>在spark streaming中做各种关联(join) 操作非常简单，一个数据流可以和另一个数据流直接关联。如：</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stream1: <span class="type">DStream</span>[<span class="type">String</span>, <span class="type">String</span>] = ...</span><br><span class="line"><span class="keyword">val</span> stream2: <span class="type">DStream</span>[<span class="type">String</span>, <span class="type">String</span>] = ...</span><br><span class="line"><span class="keyword">val</span> joinedStream = stream1.join(stream2) </span><br></pre></td></tr></tbody></table></figure>

<p>上面代码中 stream1 的每个批次中的RDD 会和stream2 相应批次中的RDD 进行join，同样，你也可以类似的使用 leftOuterJoin，rightOuterJoin,fullOuterJoin 等，此外，你还可以基于窗口来join 不同的数据流,其实实现也很简单，例如</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> windowedStream1 = stream1.window(<span class="type">Seconds</span>(<span class="number">20</span>))</span><br><span class="line"><span class="keyword">val</span> windowedStream2 = stream2.window(<span class="type">Minutes</span>(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> joinedStream = windowedStream1.join(windowedStream2)</span><br></pre></td></tr></tbody></table></figure>



<h4 id="3-3-2-数据流与数据集做关联"><a href="#3-3-2-数据流与数据集做关联" class="headerlink" title="3.3.2 数据流与数据集做关联"></a>3.3.2 数据流与数据集做关联</h4><p>其实这种情况已经在前面的Dstream.transform算子中介绍过来,这里再举个基于滑动窗口的例子.</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataset: <span class="type">RDD</span>[<span class="type">String</span>, <span class="type">String</span>] = ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> windowedStream = stream.window(<span class="type">Seconds</span>(<span class="number">20</span>))...</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> joinedStream = windowedStream.transform { rdd =&gt; rdd.join(dataset) }</span><br></pre></td></tr></tbody></table></figure>

<p> 实际上，上面代码里，你可以动态地该表join 的数据集(dataset)，传给tranform 算子的操作函数会在每个批次重新求值，所以每次该函数都会用最新的dataset值，所以不同批次间 你可以改变dataset的值</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.{<span class="type">Seconds</span>, <span class="type">StreamingContext</span>}</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by hnbia on 2017/4/5.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreaming_Window_DataFrame</span> </span>{</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = {</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = {</span><br><span class="line">      println(<span class="string">"开始 流处理 。 。 。"</span>)</span><br><span class="line">      <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"SparkStreamingDemo1"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">      <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">      <span class="comment">//设置保存点 用于故障恢复</span></span><br><span class="line">      <span class="comment">//ssc.checkpoint("/tmp/spark/checkpoint")</span></span><br><span class="line">      ssc.checkpoint(<span class="string">"D:\\spark"</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">//创建离散流，</span></span><br><span class="line">      <span class="keyword">val</span> streamLines = ssc.socketTextStream(<span class="string">"master1"</span>, <span class="number">9999</span>,<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">      streamLines</span><br><span class="line">        .window(<span class="type">Seconds</span>(<span class="number">6000</span>),<span class="type">Seconds</span>(<span class="number">10</span>))</span><br><span class="line">        .flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        .foreachRDD(rdd=&gt;{</span><br><span class="line">          <span class="keyword">val</span> sess = <span class="type">SparkSession</span>.builder().config(rdd.sparkContext.getConf).getOrCreate()</span><br><span class="line">          <span class="keyword">import</span> sess.implicits._</span><br><span class="line">          <span class="keyword">val</span> df = rdd.toDF(<span class="string">"word"</span>)</span><br><span class="line">          df.createGlobalTempView(<span class="string">"words"</span>)</span><br><span class="line">          sess.sql(<span class="string">"select word,count(word) as c from words group by word order by c desc"</span>).show()</span><br><span class="line">        })</span><br><span class="line">      <span class="comment">// 启动流计算</span></span><br><span class="line">      ssc.start()</span><br><span class="line">      <span class="comment">// 等待结束</span></span><br><span class="line">      ssc.awaitTermination()</span><br><span class="line">    }</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>



<h3 id="3-4-输出算子"><a href="#3-4-输出算子" class="headerlink" title="3.4 输出算子"></a>3.4 输出算子</h3><p>输出算子可以将DStream的数据推送到外部系统，如: 数据库或者文件系统。因为输出算子将最终完成转换的数据输出到外部系统，因此只有输出算组调用时，才会真正出发Dstream transformation 算子的真正执行，这一点类似于RDD的action 算子。目前支持的输出算子如下</p>
<table>
<thead>
<tr>
<th>输出算子</th>
<th>用途</th>
</tr>
</thead>
<tbody><tr>
<td>print()</td>
<td>在 driver 节点上打印DStream每个批次中的头十个元素</td>
</tr>
<tr>
<td>saveAsTextFiles(prefix,  [suffix])</td>
<td>将DStream的内容保存到文本文件。  每个批次一个文件，各文件命名规则为  “prefix-TIME_IN_MS[.suffix]”</td>
</tr>
<tr>
<td>saveAsObjectFiles(prefix,  [suffix])</td>
<td>将DStream内容以序列化Java对象的形式保存到顺序文件中。  每个批次一个文件，各文件命名规则为  “prefix-TIME_IN_MS[.suffix]”Python API 暂不支持Python</td>
</tr>
<tr>
<td>saveAsHadoopFiles(prefix,  [suffix])</td>
<td>将DStream内容保存到Hadoop文件中。  每个批次一个文件，各文件命名规则为  “prefix-TIME_IN_MS[.suffix]”Python API 暂不支持Python</td>
</tr>
<tr>
<td>foreachRDD(func)</td>
<td>这是最通用的输出算子了，该算子接收一个函数func，func将作用于DStream的每个RDD上。  func应该实现将每个RDD的数据推到外部系统中，比如：保存到文件或者写到数据库中。  注意，func函数是在streaming应用的驱动器进程中执行的，所以如果其中包含RDD的action算子，就会触发对DStream中RDDs的实际计算过程。</td>
</tr>
</tbody></table>
<p>Dstream.foreachRDD是一个非常强大的原生工具函数，用户可以基于此算子将Dstream 数据推送到外部系统中，不过用户需要了解如何正确而高效的使用这个工具，一下举了些常见的错误,</p>
<p>通常，对外部系统写入数据需要一些链接对象(如: 远程server的TCP链接等)，以便发送数据给远程系统，因此，开发人员可能会不经意地在Spark驱动器(driver)进程中创建一个链接对象，然后又视图在Spark worker节点上使用这个链接，如下所示</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dstream.foreachRDD { rdd =&gt;</span><br><span class="line">  <span class="keyword">val</span> connection = createNewConnection()  <span class="comment">// 这行在驱动器（driver）进程执行</span></span><br><span class="line">  rdd.foreach { record =&gt;</span><br><span class="line">    connection.send(record) <span class="comment">// 而这行将在worker节点上执行</span></span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p> 这段代码是错误的，因为它需要把链接对象序列化，再动驱动器节点发送到worker节点，而这些链接对象通常都是不能夸节点(机器)传递的，比如，链接对象通常都不能序列化，或者在另一个进程中反序列化后再次初始化等.(连接对象通常需要初始化，因此从驱动节点发到worker节点可能需要重新初始化) 解决此类错误的办法就是在worker节点上创建连接对象,</p>
<p>然而有些开发人员可能会走另一个极端，为每一条记录都创建一个连接对象，例如:</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dstream.foreachRDD { rdd =&gt;</span><br><span class="line">  rdd.foreach { record =&gt;</span><br><span class="line">    <span class="keyword">val</span> connection = createNewConnection()</span><br><span class="line">    connection.send(record)</span><br><span class="line">    connection.close()</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>一般来说，连接对象是有时间和资源开销限制的，因此，对每条记录都进行一次连接对象的创建和销毁会增加很多不必要的开销，同事也大大减小了系统的吞吐量,而一个比较好的解决方案就是使用rdd.foreachPartition - 为RDD的每个分区创建一个单独的连接对象 ，示例如下:</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dstream.foreachRDD { rdd =&gt;</span><br><span class="line">  rdd.foreachPartition { partitionOfRecords =&gt;</span><br><span class="line">    <span class="keyword">val</span> connection = createNewConnection()</span><br><span class="line">    partitionOfRecords.foreach(record =&gt; connection.send(record))</span><br><span class="line">    connection.close()</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>这样一来,连接对象的创建开销就摊到很多条记录上了.</p>
<p>最后还有一个更优的办法，就是在很多个RDD批次之间复用连接对象，开发者可以维护一个静态连接池来保存对象，以便在不同批次的多个RDD之前共享同一组连接对象,示例如下:</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dstream.foreachRDD { rdd =&gt;</span><br><span class="line">  rdd.foreachPartition { partitionOfRecords =&gt;</span><br><span class="line">    <span class="comment">// ConnectionPool 是一个静态的、懒惰初始化的连接池</span></span><br><span class="line">    <span class="keyword">val</span> connection = <span class="type">ConnectionPool</span>.getConnection()</span><br><span class="line">    partitionOfRecords.foreach(record =&gt; connection.send(record))</span><br><span class="line">    <span class="type">ConnectionPool</span>.returnConnection(connection)  <span class="comment">// 将连接返还给连接池，以便后续复用之</span></span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>注意连接池中的连接应该是懒惰创建的，并且有确定的超时时间，超时后自动销毁，这个实现应该是目前发送数最高的实现方式.</p>
<p>其他要点:</p>
<ul>
<li>DStream的转化执行也是懒惰的，需要输出算子来出发，这一点和RDD的懒惰执行由action算子触发很类似，特别的，DStream输出算子中包含RDD的action算子会强制触发对所有接收数据的处理。因此，如果你的Streaming应用中没有输出算子，或者你用了dstream.foreachRdd(func)     缺没有在func中调用RDD的action 算子，那么这个应用只会接收数据，而不会处理数据，接收都的数据最后只是被简单的丢弃掉了，</li>
<li>默认的，输出算子只能一次执行一次,且按照他们在应用程序中定义的顺序执行</li>
</ul>
<h3 id="3-5-DataFrame和SQL相关算子"><a href="#3-5-DataFrame和SQL相关算子" class="headerlink" title="3.5 DataFrame和SQL相关算子"></a>3.5 DataFrame和SQL相关算子</h3><p>在Streaming 应用中可以调用 <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/sql-programming-guide.html">DataFrames and SQL </a>来处理流式数据。开发者可以通过StreamingContext 中的SparkContext对象来创建SQLContext ，并且，开发者需要确保一旦驱动器(driver) 故障回复后，该SQLContext对象能重新创建出来，同样，你还是可以使用懒惰创建的单利模式来实例化SQLContext，如下面代码所示，我们将最开始那个例子多了一些修改，使用DataFeame 和SQL来统计单词数，其实就是将每个RDD都转化成一个RDDFrame，然后注册成临时表，再用SQL查询这些临时表</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** streaming应用中调用DataFrame算子 */</span></span><br><span class="line"><span class="keyword">val</span> words: <span class="type">DStream</span>[<span class="type">String</span>] = ...</span><br><span class="line">words.foreachRDD { rdd =&gt;</span><br><span class="line">  <span class="comment">// 获得SQLContext单例</span></span><br><span class="line">  <span class="keyword">val</span> sqlContext = <span class="type">SQLContext</span>.getOrCreate(rdd.sparkContext)</span><br><span class="line">  <span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line">  <span class="comment">// 将RDD[String] 转为 DataFrame</span></span><br><span class="line">  <span class="keyword">val</span> wordsDataFrame = rdd.toDF(<span class="string">"word"</span>)</span><br><span class="line">  <span class="comment">// DataFrame注册为临时表</span></span><br><span class="line">  wordsDataFrame.registerTempTable(<span class="string">"words"</span>)</span><br><span class="line">  <span class="comment">// 再用SQL语句查询，并打印出来</span></span><br><span class="line">  <span class="keyword">val</span> wordCountsDataFrame =</span><br><span class="line">    sqlContext.sql(<span class="string">"select word, count(*) as total from words group by word"</span>)</span><br><span class="line">  wordCountsDataFrame.show()</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>也可以在其他线程里执行SQL查询(异步查询，即: 执行SQL查询的线程和运行StreamingContext的线程不同)。不过这种情况下，你需要确保查询的时候StreamingContext没有把所需的数据丢弃掉，否则StreamingContext有可能已经将老的RDD数据丢弃掉，那么异步查询的sql语句也可能不发得到查询结果,</p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">hnbian</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://www.hnbian.cn/posts/f66955d3.html">https://www.hnbian.cn/posts/f66955d3.html</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">hnbian</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/spark/">
                                    <span class="chip bg-color">spark</span>
                                </a>
                            
                                <a href="/tags/spark-streaming/">
                                    <span class="chip bg-color">spark streaming</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    
        <style>
    .valine-card {
        margin: 1.5rem auto;
    }

    .valine-card .card-content {
        padding: 20px 20px 5px 20px;
    }

    #vcomments textarea {
        box-sizing: border-box;
        background: url("/medias/comment_bg.png") 100% 100% no-repeat;
    }

    #vcomments p {
        margin: 2px 2px 10px;
        font-size: 1.05rem;
        line-height: 1.78rem;
    }

    #vcomments blockquote p {
        text-indent: 0.2rem;
    }

    #vcomments a {
        padding: 0 2px;
        color: #4cbf30;
        font-weight: 500;
        text-decoration: none;
    }

    #vcomments img {
        max-width: 100%;
        height: auto;
        cursor: pointer;
    }

    #vcomments ol li {
        list-style-type: decimal;
    }

    #vcomments ol,
    ul {
        display: block;
        padding-left: 2em;
        word-spacing: 0.05rem;
    }

    #vcomments ul li,
    ol li {
        display: list-item;
        line-height: 1.8rem;
        font-size: 1rem;
    }

    #vcomments ul li {
        list-style-type: disc;
    }

    #vcomments ul ul li {
        list-style-type: circle;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    #vcomments table, th, td {
        border: 0;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments h1 {
        font-size: 1.85rem;
        font-weight: bold;
        line-height: 2.2rem;
    }

    #vcomments h2 {
        font-size: 1.65rem;
        font-weight: bold;
        line-height: 1.9rem;
    }

    #vcomments h3 {
        font-size: 1.45rem;
        font-weight: bold;
        line-height: 1.7rem;
    }

    #vcomments h4 {
        font-size: 1.25rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    #vcomments h5 {
        font-size: 1.1rem;
        font-weight: bold;
        line-height: 1.4rem;
    }

    #vcomments h6 {
        font-size: 1rem;
        line-height: 1.3rem;
    }

    #vcomments p {
        font-size: 1rem;
        line-height: 1.5rem;
    }

    #vcomments hr {
        margin: 12px 0;
        border: 0;
        border-top: 1px solid #ccc;
    }

    #vcomments blockquote {
        margin: 15px 0;
        border-left: 5px solid #42b983;
        padding: 1rem 0.8rem 0.3rem 0.8rem;
        color: #666;
        background-color: rgba(66, 185, 131, .1);
    }

    #vcomments pre {
        font-family: monospace, monospace;
        padding: 1.2em;
        margin: .5em 0;
        background: #272822;
        overflow: auto;
        border-radius: 0.3em;
        tab-size: 4;
    }

    #vcomments code {
        font-family: monospace, monospace;
        padding: 1px 3px;
        font-size: 0.92rem;
        color: #e96900;
        background-color: #f8f8f8;
        border-radius: 2px;
    }

    #vcomments pre code {
        font-family: monospace, monospace;
        padding: 0;
        color: #e8eaf6;
        background-color: #272822;
    }

    #vcomments pre[class*="language-"] {
        padding: 1.2em;
        margin: .5em 0;
    }

    #vcomments code[class*="language-"],
    pre[class*="language-"] {
        color: #e8eaf6;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }

    #vcomments b,
    strong {
        font-weight: bold;
    }

    #vcomments dfn {
        font-style: italic;
    }

    #vcomments small {
        font-size: 85%;
    }

    #vcomments cite {
        font-style: normal;
    }

    #vcomments mark {
        background-color: #fcf8e3;
        padding: .2em;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }
</style>

<div class="card valine-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; padding-left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="vcomments" class="card-content" style="display: grid">
    </div>
</div>

<script src="/libs/valine/av-min.js"></script>
<script src="/libs/valine/Valine.min.js"></script>
<script>
    new Valine({
        el: '#vcomments',
        appId: 'h1rND3bHC0OqFwzm1UfPQpaY-gzGzoHsz',
        appKey: 'd7uboDt2WLV2HJCEMHkkLuU4',
        serverURLs: '',
        notify: 'false' === 'true',
        verify: 'false' === 'true',
        visitor: 'true' === 'true',
        avatar: 'identicon',
        pageSize: '12',
        lang: 'zh-cn',
        placeholder: 'just go go'
    });
</script>

<!--酷Q推送-->


    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/posts/91e47788.html">
                    <div class="card-image">
                        
                        <img src="https://images.hnbian.cn/Fj2xo508sasH6ctH1DPZxYO0rrP1" class="responsive-img" alt="Spark Streaming 3. 数据广播与检查点">
                        
                        <span class="card-title">Spark Streaming 3. 数据广播与检查点</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2018-03-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/spark/" class="post-category">
                                    spark
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/spark/">
                        <span class="chip bg-color">spark</span>
                    </a>
                    
                    <a href="/tags/spark-streaming/">
                        <span class="chip bg-color">spark streaming</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/posts/667711ec.html">
                    <div class="card-image">
                        
                        <img src="https://images.hnbian.cn/Ftt1O6UxuoToNPTSUDD4IVs0Spnj" class="responsive-img" alt="Spark Streaming 1. 介绍">
                        
                        <span class="card-title">Spark Streaming 1. 介绍</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2018-02-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/spark/" class="post-category">
                                    spark
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/spark/">
                        <span class="chip bg-color">spark</span>
                    </a>
                    
                    <a href="/tags/spark-streaming/">
                        <span class="chip bg-color">spark streaming</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE' || selection.getRangeAt(0).commonAncestorContainer.nodeName === 'CODE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: hnbian<br />'
            + '文章作者: hnbian<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4, h5'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2018-2025</span>
            
            <a href="/about" target="_blank">hnbian</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
                <span id="translate">|&nbsp;繁/简：</span><a id="translateLink" href="javascript:translatePage();">繁</a>
            
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2018";
                        var startMonth = "2";
                        var startDate = "8";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
                <span id="icp"><img src="/medias/icp.png"
                                    style="vertical-align: text-bottom;"/>
                <a href="https://beian.miit.gov.cn" target="_blank">粤ICP备18156920号-2</a>
            </span>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/hnbian" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:hnbian@126.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>













</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
        <script type="text/javascript" src="/js/tw_cn.js"></script>
        <script type="text/javascript">
          var defaultEncoding = 2; //网站编写字体是否繁体，1-繁体，2-简体
          var translateDelay = 0; //延迟时间,若不在前, 要设定延迟翻译时间, 如100表示100ms,默认为0
          var cookieDomain = "https://www.hnbian.cn"; //Cookie地址, 一定要设定, 通常为你的网址
          var msgToTraditionalChinese = "繁"; //此处可以更改为你想要显示的文字
          var msgToSimplifiedChinese = "简"; //同上，但两处均不建议更改
          var translateButtonId = "translateLink"; //默认互换id
          translateInitilization();
        </script>
    
    
    

    <!-- 雪花特效 -->
     
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/libs/others/snow.js"><\/script>');
            }
        </script>
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
